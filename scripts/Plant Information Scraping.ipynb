{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f780ea3c-6d29-4b6b-ba37-05ed1fc3d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da00e68-2801-4186-9d77-13ca61d30564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of plant names and corresponding FAO codes\n",
    "plants = [\n",
    "    (\"Apples\", 1407), (\"Bananas\", 2483), (\"Barley\", 1232), (\"Cotton\", 1158), (\"Dates\", 1673),\n",
    "    (\"Eggplants\", 1965), (\"Grapes\", 2160), (\"Maize (Corn)\", 2175), (\"Mango\", 1416), (\"Olives\", 1553),\n",
    "    (\"Onions and Shallots\", 364), (\"Oranges\", 720), (\"Peanut\", 2199), (\"Potatoes\", 1971), (\"Rice\", 1574),\n",
    "    (\"Sorghum\", 48747), (\"Soybean\", 1150), (\"Sugar Beet\", 514), (\"Sugar Cane\", 1884), (\"Sunflowerseed\", 1191),\n",
    "    (\"Tomatoes\", 1379), (\"Watermelon\", 708), (\"Wheat\", 2114)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5546d008-f930-4182-9da0-641ec19d6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for FAO EcoCrop database\n",
    "def get_fao_url(plant_id):\n",
    "    return f\"https://ecocrop.apps.fao.org/ecocrop/srv/en/dataSheet?id={plant_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "694dc4ff-451b-4986-a1f8-cc5a3ee6026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape data\n",
    "def scrape_fao_data(plant_name, plant_id):\n",
    "    url = get_fao_url(plant_id)\n",
    "    response = requests.get(url)\n",
    "    with open(f'htmls/{plant_name}_{plant_id}.html', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {plant_name} (ID: {plant_id})\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting plant scientific name\n",
    "    sci_name = soup.find('h2').text.strip() if soup.find('h2') else None\n",
    "\n",
    "    # Extracting attributes from the \"Description\" table\n",
    "    desc_table = soup.find_all('table')[0]\n",
    "    desc_data = {}\n",
    "    for row in desc_table.find_all('tr')[1:]:  # Skip header row\n",
    "        keys = row.find_all('th')\n",
    "        values = row.find_all('td')\n",
    "        for key, value in zip(keys, values):\n",
    "            desc_data['Desc ' + key.text.strip()] = value.text.strip()\n",
    "\n",
    "    # Extracting attributes from the \"Ecology\" table\n",
    "    eco_data = {}\n",
    "    eco_table = soup.find_all('table')[1]\n",
    "    \n",
    "    # high row\n",
    "    row = eco_table.find_all('tr')[2]\n",
    "    key = row.find_all('th')[0]\n",
    "    eco_data['Eco ' + key.text.strip() + ' Optimal'] = row.find_all('td')[5].text.strip()\n",
    "    eco_data['Eco ' + key.text.strip() + ' Absolute'] = row.find_all('td')[6].text.strip()\n",
    "\n",
    "    # middle rows\n",
    "    for row in eco_table.find_all('tr')[3:8]:  # Skip headers\n",
    "        # left half\n",
    "        key = row.find_all('th')[0]\n",
    "        eco_data['Eco ' + key.text.strip() + ' Optimal Min'] = row.find_all('td')[0].text.strip()\n",
    "        eco_data['Eco ' + key.text.strip() + ' Optimal Max'] = row.find_all('td')[1].text.strip()\n",
    "        eco_data['Eco ' + key.text.strip() + ' Absolute Min'] = row.find_all('td')[2].text.strip()\n",
    "        eco_data['Eco ' + key.text.strip() + ' Absolute Max'] = row.find_all('td')[3].text.strip()\n",
    "    \n",
    "        # right half\n",
    "        key = row.find_all('th')[1]\n",
    "        eco_data['Eco ' + key.text.strip() + ' Optimal'] = row.find_all('td')[4].text.strip()\n",
    "        eco_data['Eco ' + key.text.strip() + ' Absolute'] = row.find_all('td')[5].text.strip()\n",
    "        \n",
    "    # low row\n",
    "    row = eco_table.find_all('tr')[8]\n",
    "    key = row.find_all('th')[0]\n",
    "    eco_data['Eco ' + key.text.strip() + ' Optimal Min'] = row.find_all('td')[0].text.strip()\n",
    "    eco_data['Eco ' + key.text.strip() + ' Optimal Max'] = row.find_all('td')[1].text.strip()\n",
    "    eco_data['Eco ' + key.text.strip() + ' Absolute Min'] = row.find_all('td')[2].text.strip()\n",
    "    eco_data['Eco ' + key.text.strip() + ' Absolute Max'] = row.find_all('td')[3].text.strip()\n",
    "\n",
    "    # Extracting the remaining attributes from the \"Ecology\" table\n",
    "    eco_table = soup.find_all('table')[2]\n",
    "\n",
    "    for row in eco_table.find_all('tr')[:3]:\n",
    "        # left half\n",
    "        key = row.find_all('th')[0]\n",
    "        eco_data['Eco ' + key.text.strip()] = row.find_all('td')[0].text.strip()\n",
    "    \n",
    "        # right half\n",
    "        key = row.find_all('th')[1]\n",
    "        eco_data['Eco ' + key.text.strip()] = row.find_all('td')[1].text.strip()\n",
    "\n",
    "    # last row\n",
    "    row = eco_table.find_all('tr')[3]\n",
    "    key = row.find_all('th')[0]\n",
    "    eco_data['Eco ' + key.text.strip()] = row.find_all('td')[0].text.strip()\n",
    "    \n",
    "    # Returning extracted data\n",
    "    return {\n",
    "        \"ID\": plant_id,\n",
    "        \"Plant Name\": plant_name,\n",
    "        \"Scientific Name\": sci_name,\n",
    "        **desc_data,\n",
    "        **eco_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "993bb034-3fe8-4f0d-b01d-a347457515cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed! CSV file saved.\n"
     ]
    }
   ],
   "source": [
    "# List to store all data\n",
    "data = []\n",
    "\n",
    "# Loop through each plant and scrape data\n",
    "for plant_name, plant_id in plants:\n",
    "    plant_data = scrape_fao_data(plant_name, plant_id)\n",
    "    if plant_data:\n",
    "        data.append(plant_data)\n",
    "    time.sleep(1)  # To avoid overwhelming the server\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe to CSV\n",
    "df.to_csv(\"D:\\\\fao_plant_data.csv\", index=False)\n",
    "\n",
    "print(\"Data extraction completed! CSV file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce831193-cdf3-4cf5-b668-e3194919d451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e821334-29ec-46ff-ac3b-9e4dc34e2e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20985503-dec0-43ef-880f-43aa4318ae77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e21046-6ec7-4819-9e66-8738641526a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
